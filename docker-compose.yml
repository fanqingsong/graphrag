version: '3.8'

services:
  neo4j:
    image: swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/neo4j:latest
    container_name: neo4j
    environment:
      - NEO4J_AUTH=neo4j/graphrag_password
      - NEO4J_PLUGINS=["graph-data-science"]
    ports:
      - "7474:7474"  # HTTP 
      - "7687:7687"  # Bolt 
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_plugins:/plugins
    networks:
      - graphrag_network
    restart: unless-stopped

  backend:
    build: 
      context: .
      dockerfile: Dockerfile.backend
    container_name: graphrag-backend
    environment:
      # OpenAI Configuration - set these in your .env or pass via docker run
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL}
      - OPENAI_MODEL=${OPENAI_MODEL}
      - OPENAI_PROXY=${OPENAI_PROXY}
      # LLM provider selection: 'openai' or 'ollama'
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      # Ollama configuration (used when LLM_PROVIDER=ollama)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://localhost:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3}
      - OLLAMA_EMBEDDING_MODEL=${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text}
      - OLLAMA_API_KEY=${OLLAMA_API_KEY:-}
      # Neo4j Configuration - connecting to the neo4j service
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      # Embedding Configuration
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-ada-002}
      - EMBEDDING_CONCURRENCY=${EMBEDDING_CONCURRENCY:-3}  # Number of parallel embedding requests (default: 3)
      # Document Processing Configuration
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-200}
      # Similarity Configuration
      - SIMILARITY_THRESHOLD=${SIMILARITY_THRESHOLD:-0.7}
      - MAX_SIMILARITY_CONNECTIONS=${MAX_SIMILARITY_CONNECTIONS:-5}
      # Application Configuration
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_UPLOAD_SIZE=${MAX_UPLOAD_SIZE:-104857600}
      # Feature Flags
      - ENABLE_ENTITY_EXTRACTION=${ENABLE_ENTITY_EXTRACTION:-true}
      - ENABLE_QUALITY_SCORING=${ENABLE_QUALITY_SCORING:-true}
      - ENABLE_DELETE_OPERATIONS=${ENABLE_DELETE_OPERATIONS:-true}
    ports:
      - "8000:8000"  # FastAPI port
    volumes:
      - ./data:/app/data  # Mount data directory for file uploads
    networks:
      - graphrag_network
    depends_on:
      - neo4j
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: graphrag-frontend
    ports:
      - "3000:3000"  # Next.js port
    environment:
      # Next.js configuration
      - NODE_ENV=production
      # API endpoint for frontend to communicate with backend
      - NEXT_PUBLIC_API_URL=http://backend:8000
    networks:
      - graphrag_network
    depends_on:
      - backend
    restart: unless-stopped

networks:
  graphrag_network:
    driver: bridge

volumes:
  neo4j_data:
  neo4j_logs:
  neo4j_plugins: